import os
from typing import List, Tuple

import numpy as np
import streamlit as st
from dotenv import load_dotenv
from openai import OpenAI
from pypdf import PdfReader

from langchain_text_splitters import RecursiveCharacterTextSplitter



# ---------- Load env ----------
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

if not OPENAI_API_KEY:
    raise RuntimeError("Î›ÎµÎ¯Ï€ÎµÎ¹ Ï„Î¿ OPENAI_API_KEY Î±Ï€ÏŒ Ï„Î¿ .env")

client = OpenAI(api_key=OPENAI_API_KEY)

def extract_text_from_pdf(file_path: str) -> str:
    reader = PdfReader(file_path)
    pdf_text = [page.extract_text() for page in reader.pages if page.extract_text() is not None]

    return "\n".join(pdf_text)


def chunk_text(text: str, chunk_size: int = 1000, chunk_overlap: int = 200) -> List[str]:
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        separators=["\n\n", "\n",".", " ", ""],
    )
    chunks = text_splitter.split_text(text)
    return chunks

import chromadb
from chromadb.utils import embedding_functions

chroma_client = chromadb.PersistentClient(path="./chroma_db")
# Î§ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î¿ÏÎ¼Îµ built-in OpenAIEmbeddingFunction Ï„Î¿Ï… Chroma
chroma_ef = embedding_functions.OpenAIEmbeddingFunction(
    api_key=OPENAI_API_KEY,
    model_name="text-embedding-3-small",
)

# Î£Ï…Î»Î»Î¿Î³Î® Î³Î¹Î± Ï„Î± Î­Î³Î³ÏÎ±Ï†Î¬ Î¼Î±Ï‚
chroma_collection = chroma_client.get_or_create_collection(
    name="rag-docs",
    embedding_function=chroma_ef,
)

def build_rag_prompt(question: str, retrieved_chunks: List[Tuple[int, str]]) -> str:

    if not retrieved_chunks:
        context_text = "No relevant context was found."
    else:
        context_lines = []
        for idx, chunk in retrieved_chunks:
            context_lines.append(f"[{idx}] {chunk}")
        context_text = "\n\n".join(context_lines)

    prompt = f"""
You are an assistant that answers ONLY based on the context provided below.

Document context:
{context_text}

User question:
{question}

Instructions:
- If the answer is not clearly derived from the context, say that you do not have enough information.
- When you use information from a specific excerpt, add its number in brackets at the end of the point, e.g. [12].
"""

    return prompt



# ---------- Simple chat function ----------
def ask_llm(message: str) -> str:
    """
    Î£Ï„Î­Î»Î½ÎµÎ¹ Î­Î½Î± Î¼Î®Î½Ï…Î¼Î± ÏƒÏ„Î¿ LLM ÎºÎ±Î¹ ÎµÏ€Î¹ÏƒÏ„ÏÎ­Ï†ÎµÎ¹ Ï„Î·Î½ Î±Ï€Î¬Î½Ï„Î·ÏƒÎ·.
    Î ÏÎ¿Ï‚ Ï„Î¿ Ï€Î±ÏÏŒÎ½ Î§Î©Î¡Î™Î£ RAG, Î±Ï€Î»Î¬ LLM.
    """
    response = client.chat.completions.create(
        model="gpt-4.1-mini",   # Î® gpt-4o-mini, Î±Î½Î¬Î»Î¿Î³Î± Ï„Î¹ Î­Ï‡ÎµÎ¹Ï‚ Î´Î¹Î±Î¸Î­ÏƒÎ¹Î¼Î¿
        messages=[
            {"role": "system", "content": "You are a helpful AI assistant. Answer in Greek."},
            {"role": "user", "content": message},
        ],
        temperature=0.4,
    )
    return response.choices[0].message.content


# ---------- Streamlit UI ----------
# -------------------------------------------
# 4. Streamlit UI
# -------------------------------------------
st.set_page_config(page_title="LLM RAG Chat Î¼Îµ Chroma", page_icon="ğŸ“š")

st.title("ğŸ“š LLM RAG Chat Î¼Îµ OpenAI + ChromaDB")
st.write(
    """
Demo ÎµÏ†Î±ÏÎ¼Î¿Î³Î® RAG:

1. Î‘Î½ÎµÎ²Î¬Î¶ÎµÎ¹Ï‚ PDF.
2. Î¤Î± chunks & embeddings Î±Ï€Î¿Î¸Î·ÎºÎµÏÎ¿Î½Ï„Î±Î¹ ÏƒÎµ ChromaDB.
3. ÎšÎ¬Î½ÎµÎ¹Ï‚ ÎµÏÏ‰Ï„Î®ÏƒÎµÎ¹Ï‚ ÎºÎ±Î¹ Î³Î¯Î½ÎµÏ„Î±Î¹ retrieval + LLM answer.
"""
)

if "messages" not in st.session_state:
    st.session_state.messages = []

# ---------- Upload & ingest ----------
st.subheader("1ï¸âƒ£ Î‘Î½Î­Î²Î±ÏƒÎµ PDF Î³Î¹Î± indexing")

uploaded_file = st.file_uploader("Î•Ï€Î¯Î»ÎµÎ¾Îµ Î­Î½Î± PDF Î­Î³Î³ÏÎ±Ï†Î¿", type=["pdf"])

if uploaded_file is not None:
    with st.spinner("Î”Î¹Î±Î²Î¬Î¶Ï‰ Ï„Î¿ PDF ÎºÎ±Î¹ Ï†Ï„Î¹Î¬Ï‡Î½Ï‰ chunks..."):
        full_text = extract_text_from_pdf(uploaded_file)
        chunks = chunk_text(full_text)

        if not chunks:
            st.error("Î”ÎµÎ½ Î²ÏÎ­Î¸Î·ÎºÎµ ÎºÎµÎ¯Î¼ÎµÎ½Î¿ ÏƒÏ„Î¿ PDF.")
        else:
            st.write(f"Î’ÏÎ­Î¸Î·ÎºÎ±Î½ **{len(chunks)}** chunks. Î¤Î± Ï€ÏÎ¿ÏƒÎ¸Î­Ï„Ï‰ ÏƒÏ„Î· Chroma ÏƒÏ…Î»Î»Î¿Î³Î®...")

            ids = []
            metadatas = []
            for i in range(len(chunks)):
                ids.append(f"{uploaded_file.name}-chunk-{i}")
                metadatas.append(
                    {
                        "source": uploaded_file.name,
                        "chunk_index": i,
                    }
                )

            chroma_collection.add(
                documents=chunks,
                ids=ids,
                metadatas=metadatas,
            )

            st.success("Î¤Î¿ Î­Î³Î³ÏÎ±Ï†Î¿ Î¼Ï€Î®ÎºÎµ ÏƒÏ„Î¿ RAG index!")
            with st.expander("Î”ÎµÎ¯Î³Î¼Î± Î±Ï€ÏŒ Ï„Î± Ï€ÏÏÏ„Î± chunks"):
                for i, ch in enumerate(chunks[:3]):
                    st.markdown(f"**Chunk {i}:**")
                    st.write(ch)

# ---------- Chat ----------
st.subheader("2ï¸âƒ£ ÎšÎ¬Î½Îµ ÎµÏÏ‰Ï„Î®ÏƒÎµÎ¹Ï‚ Ï€Î¬Î½Ï‰ ÏƒÏ„Î± indexed Î­Î³Î³ÏÎ±Ï†Î±")

for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

user_input = st.chat_input("Î“ÏÎ¬ÏˆÎµ Ï„Î·Î½ ÎµÏÏÏ„Î·ÏƒÎ® ÏƒÎ¿Ï…...")

if user_input:
    st.session_state.messages.append({"role": "user", "content": user_input})
    with st.chat_message("user"):
        st.markdown(user_input)

    # ÎˆÎ»ÎµÎ³Ï‡Î¿Ï‚ Î±Î½ Ï…Ï€Î¬ÏÏ‡Î¿Ï…Î½ ÎºÎ±Î¸ÏŒÎ»Î¿Ï… docs ÏƒÏ„Î· ÏƒÏ…Î»Î»Î¿Î³Î®
    if chroma_collection.count() == 0:
        msg = "Î”ÎµÎ½ Ï…Ï€Î¬ÏÏ‡Î¿Ï…Î½ documents ÏƒÏ„Î· Chroma. Î‘Î½Î­Î²Î±ÏƒÎµ Ï€ÏÏÏ„Î± Î­Î½Î± PDF."
        with st.chat_message("assistant"):
            st.markdown(msg)
        st.session_state.messages.append({"role": "assistant", "content": msg})
    else:
        with st.spinner("Î¨Î¬Ï‡Î½Ï‰ ÏƒÏ‡ÎµÏ„Î¹ÎºÏŒ context ÏƒÏ„Î· Chroma..."):
            results = chroma_collection.query(
                query_texts=[user_input],
                n_results=4,
            )

        docs = results["documents"][0]
        metas = results["metadatas"][0]

        retrieved = []
        for doc, meta in zip(docs, metas):
            idx = meta.get("chunk_index", -1)
            retrieved.append((idx, doc))

        rag_prompt = build_rag_prompt(user_input, retrieved)

        with st.chat_message("assistant"):
            with st.spinner("Î£ÎºÎ­Ï†Ï„Î¿Î¼Î±Î¹ Î¼Îµ Î²Î¬ÏƒÎ· Ï„Î± Î­Î³Î³ÏÎ±Ï†Î¬ ÏƒÎ¿Ï…..."):
                answer = ask_llm(rag_prompt)
                st.markdown(answer)

            if retrieved:
                with st.expander("Context Ï€Î¿Ï… Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î®Î¸Î·ÎºÎµ Î±Ï€ÏŒ Chroma"):
                    for idx, chunk in retrieved:
                        st.markdown(f"**Chunk [{idx}]:**")
                        st.write(chunk)

        st.session_state.messages.append({"role": "assistant", "content": answer})